{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465a6aef",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "**Suárez Pérez Juan Pablo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa17b4",
   "metadata": {},
   "source": [
    "## Text Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af1ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from nlp_functions import text_processing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530b2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the corpus\n",
    "tp.clean_corpus('./../EXCELSIOR_100_files/', './new_corpus/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fb09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean corpus\n",
    "text = tp.get_clean_text('./new_corpus/clean_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a348f1",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6762636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "words = tp.word_tokenize(text)\n",
    "sents = tp.sentence_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a02f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete stop words\n",
    "new_sents = tp.delete_stop_words_sents(sents, './nlp_functions/stopwords_and_lemmas/stopwords_es.txt')\n",
    "new_words = tp.delete_stop_words(words, './nlp_functions/stopwords_and_lemmas/stopwords_es.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc0ba0",
   "metadata": {},
   "source": [
    "Dosnt matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias...\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc791b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestra función de normalización de texto...\n",
    "def normalize(path = './../Text_Extraction/EXCELSIOR_100_files/'):\n",
    "    # Obtenemos el corpus del directorio...\n",
    "    corpus = PlaintextCorpusReader(path, '.*')\n",
    "    # Obtenemos la lista de archivos que pertenecen a nuestro corpus y los añadimos a una lista...\n",
    "    file_list = corpus.fileids()\n",
    "    # Juntamos todo el texto de todos los archivos...\n",
    "    all_text = ''\n",
    "    # Obtenemos el texto de cada uno de nuestros archivos...\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Removemos las etiquetas html...\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    # Convertimos todo el texto a minúsculas...\n",
    "    clean_text = clean_text.lower()\n",
    "    # Tokenizamos el texto...\n",
    "    words = clean_text.split()\n",
    "    # Obtenemos unicamente las palabras alfabéticas...\n",
    "    alphabetic_words = []\n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Quitamos las Stop words...\n",
    "    with open('./stopwords_es.txt', encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "    # Obtenemos la lista de palabras despues de eliminar las stop_words...\n",
    "    final_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "    print('Fin de la normalización...')\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32872f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematize(text):\n",
    "    lemmas = dict()\n",
    "    with open('./generate.txt', encoding = 'latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [w.strip() for w in lines]\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    lemmatized_text = []\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    words = normalize()\n",
    "    print(f'Algunas palabras despues de la normalización: \\n{words[:200]}')\n",
    "except Exception as e:\n",
    "    print('Ocurrió el siguiente error: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    new_words = lematize(words)\n",
    "    print(f'Algunas palabras despues de la normalización: \\n{new_words[:200]}')\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9e89",
   "metadata": {},
   "source": [
    "## Representación Númerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a705f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import word_association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7949e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word_association.make_vocabulary(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364efe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d23618",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = word_association.create_vector_space(new_words, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
